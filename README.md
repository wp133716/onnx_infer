# onnx_infer
Use onnxruntime to deploy the model in the c++ for inference
